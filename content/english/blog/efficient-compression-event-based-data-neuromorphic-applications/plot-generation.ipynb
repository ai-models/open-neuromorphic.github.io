{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: expelliarmus in /Users/fab/miniconda3/lib/python3.11/site-packages (1.1.12)\n",
      "Requirement already satisfied: aedat in /Users/fab/miniconda3/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: event_stream in /Users/fab/miniconda3/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: h5py in /Users/fab/miniconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: aestream in /Users/fab/miniconda3/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: loris in /Users/fab/miniconda3/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: brotli in /Users/fab/miniconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: plotly in /Users/fab/miniconda3/lib/python3.11/site-packages (5.17.0)\n",
      "Requirement already satisfied: kaleido in /Users/fab/miniconda3/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: dv_processing in /Users/fab/miniconda3/lib/python3.11/site-packages (1.7.9)\n",
      "Requirement already satisfied: numpy in /Users/fab/miniconda3/lib/python3.11/site-packages (from expelliarmus) (1.25.2)\n",
      "Requirement already satisfied: pysdl2-dll in /Users/fab/miniconda3/lib/python3.11/site-packages (from aestream) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/fab/miniconda3/lib/python3.11/site-packages (from loris) (4.65.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/fab/miniconda3/lib/python3.11/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /Users/fab/miniconda3/lib/python3.11/site-packages (from plotly) (23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "NO SPECK module 'aestream.aestream_ext' has no attribute 'SpeckInput'\n"
     ]
    }
   ],
   "source": [
    "%pip install expelliarmus aedat event_stream h5py aestream loris brotli plotly kaleido dv_processing\n",
    "\n",
    "from expelliarmus import Wizard\n",
    "import aedat\n",
    "import event_stream\n",
    "import gc\n",
    "import hashlib\n",
    "import h5py\n",
    "import aestream\n",
    "import numpy as np\n",
    "import timeit\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import loris\n",
    "import brotli\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fname = \"driving_sample\"\n",
    "fname = \"construction\"  # use this one if you want to include aedat and eventstream benchmarks\n",
    "\n",
    "# where to download and generate all the benchmark data\n",
    "folder = Path(\"data/file-benchmark\")\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# key is the name of the encoding, value is the file name ending\n",
    "extension_map = {\n",
    "    \"aedat\": \".aedat4\",\n",
    "    \"dat\": \".dat\",\n",
    "    \"evt2\": \"_evt2.raw\",\n",
    "    \"evt3\": \"_evt3.raw\",\n",
    "    \"hdf5\": \".hdf5\",\n",
    "    \"hdf5_lzf\": \"_lzf.hdf5\",\n",
    "    \"hdf5_gzip\": \"_gzip.hdf5\",\n",
    "    \"numpy\": \".npy\",\n",
    "    \"loris\": \".es\",\n",
    "    \"eventstream\": \".es\",\n",
    "    \"brotli\": \".bin.br\",\n",
    "    \"undr_numpy\": \".dvs\",\n",
    "    \"undr_brotli_11\": \".11.dvs.br\",\n",
    "    \"undr_brotli_6\": \".6.dvs.br\",\n",
    "    \"undr_brotli_1\": \".1.dvs.br\",\n",
    "}\n",
    "get_fpath = lambda encoding: f\"{folder}/{fname}{extension_map[encoding]}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the 'base' files\n",
    "These are the files with the original data, which will be loaded and then converted to all other formats under test. Currently you can choose between events from a Prophesee raw evt3 or an aedat4 sample file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file_from_url(file_path, url):\n",
    "    print(f\"Downloading file to {file_path}... \")\n",
    "    r = requests.get(\n",
    "        url,\n",
    "        allow_redirects=True,\n",
    "    )\n",
    "    with open(f\"{file_path}.download\", \"wb\") as file:\n",
    "        file.write(r.content)\n",
    "    r.raise_for_status()\n",
    "    os.rename(f\"{file_path}.download\", file_path)\n",
    "    print(\"done!\")\n",
    "\n",
    "\n",
    "if fname == \"driving_sample\":\n",
    "    fpath = get_fpath(\"evt3\")\n",
    "    if not Path(fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            fpath, \"https://dataset.prophesee.ai/index.php/s/nVcLLdWAnNzrmII/download\"\n",
    "        )\n",
    "    wizard = Wizard(encoding=\"evt3\")\n",
    "    events_ti8_xi2_yi2_pu1 = wizard.read(fpath)\n",
    "\n",
    "\n",
    "if fname == \"construction\":\n",
    "    aedat_fpath = get_fpath(\"aedat\")\n",
    "    if not Path(aedat_fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            aedat_fpath,\n",
    "            \"https://cloudstor.aarnet.edu.au/plus/s/ORQ2oOz9NfwiHLZ/download?path=%2F&files=construction.aedat4\",\n",
    "        )\n",
    "    decoder = aedat.Decoder(aedat_fpath) # type: ignore\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for stream in decoder.id_to_stream().values():\n",
    "        if stream[\"type\"] == \"events\":\n",
    "            width = stream[\"width\"]\n",
    "            height = stream[\"height\"]\n",
    "            break\n",
    "    assert width != 0\n",
    "    assert height != 0\n",
    "    events_tu8_xu2_yu2_onb = np.concatenate(\n",
    "        [packet[\"events\"] for packet in decoder if \"events\" in packet]\n",
    "    )\n",
    "    assert np.count_nonzero(np.diff(events_tu8_xu2_yu2_onb[\"t\"].astype(\"<i8\")) < 0) == 0\n",
    "    events_tu8_xu2_yu2_onb[\"t\"] -= events_tu8_xu2_yu2_onb[\"t\"][0]\n",
    "    events_ti8_xi2_yi2_pu1 = events_tu8_xu2_yu2_onb.astype(\n",
    "        np.dtype([(\"t\", \"<i8\"), (\"x\", \"<i2\"), (\"y\", \"<i2\"), (\"p\", \"u1\")], align=True)\n",
    "    )\n",
    "    events_tu8_xu2_yu2_pu1 = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all comparison files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evt2 and dat\n",
    "raw_encodings = [\"dat\", \"evt2\", \"evt3\"]\n",
    "for encoding in raw_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        wizard = Wizard(encoding=encoding)\n",
    "        wizard.save(fpath=fpath, arr=events_ti8_xi2_yi2_pu1)\n",
    "\n",
    "# variants of hdf5\n",
    "hdf5_encodings = [\"hdf5\", \"hdf5_lzf\", \"hdf5_gzip\"]\n",
    "for encoding in hdf5_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        with h5py.File(fpath, \"w\") as fp:\n",
    "            print(f\"Generating {fpath}.\")\n",
    "            dataset_dict = dict(\n",
    "                name=\"events\",\n",
    "                shape=events_ti8_xi2_yi2_pu1.shape,\n",
    "                dtype=events_ti8_xi2_yi2_pu1.dtype,\n",
    "                data=events_ti8_xi2_yi2_pu1,\n",
    "            )\n",
    "            if encoding == \"hdf5\":\n",
    "                fp.create_dataset(**dataset_dict)\n",
    "            elif encoding == \"hdf5_lzf\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"lzf\")\n",
    "            elif encoding == \"hdf5_gzip\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"gzip\")\n",
    "\n",
    "# Event Stream\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    with event_stream.Encoder(fpath, \"dvs\", width, height) as encoder:\n",
    "        encoder.write(events_tu8_xu2_yu2_onb)\n",
    "\n",
    "# numpy (pickle)\n",
    "fpath = get_fpath(\"numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    np.save(fpath, events_ti8_xi2_yi2_pu1, allow_pickle=True)\n",
    "\n",
    "# numpy (UNDR)\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    events_tu8_xu2_yu2_pu1.tofile(fpath)\n",
    "\n",
    "# brotli (UNDR)\n",
    "brotli_qualities = [1, 6, 11]\n",
    "with open(get_fpath(\"undr_numpy\"), \"rb\") as uncompressed_file:\n",
    "    uncompressed_bytes = uncompressed_file.read()\n",
    "for quality in brotli_qualities:\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        compressed_bytes = brotli.compress(uncompressed_bytes, quality=quality)\n",
    "        with open(fpath, \"wb\") as compressed_file:\n",
    "            compressed_file.write(compressed_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aedat4\n",
    "# we do this in order to exclude IMU events that are part of the original file\n",
    "# see here for how to install dv https://dv-processing.inivation.com/rel_1.7/installation.html\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    import dv_processing as dv\n",
    "    from tqdm.notebook import tqdm\n",
    "    store = dv.EventStore()\n",
    "\n",
    "    aedat_compatible_events = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", int), (\"x\", int), (\"y\", int), (\"p\", bool)]))\n",
    "    for event in tqdm(aedat_compatible_events):\n",
    "        store.push_back(timestamp=event[\"t\"], x=event[\"x\"], y=event[\"y\"], polarity=event[\"p\"])\n",
    "\n",
    "    resolution = (aedat_compatible_events['x'].max()+1, aedat_compatible_events['y'].max()+1)\n",
    "    config = dv.io.MonoCameraWriter.EventOnlyConfig(\"DVXplorer_sample\", resolution)\n",
    "    writer = dv.io.MonoCameraWriter(fpath, config)\n",
    "    writer.writeEvents(store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPEAT = 5\n",
    "get_fsize_MiB = lambda fpath: round(fpath.stat().st_size / (1024 * 1024))\n",
    "\n",
    "def hash(events: np.ndarray, normalize_time: bool = False) -> str:\n",
    "    if normalize_time:\n",
    "        events[\"t\"] -= events[\"t\"][0]\n",
    "    result = hashlib.sha3_224()\n",
    "    result.update(events.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"on\", \"?\")])).tobytes())\n",
    "    return result.hexdigest()\n",
    "\n",
    "reference_hash = hash(events_tu8_xu2_yu2_onb)\n",
    "number_of_events = len(events_tu8_xu2_yu2_onb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete variables to minimize memory usage before the benchmarks\n",
    "\n",
    "del events_tu8_xu2_yu2_onb\n",
    "del events_ti8_xi2_yi2_pu1\n",
    "del events_tu8_xu2_yu2_pu1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking expelliarmus (dat).\n",
      "Benchmarking expelliarmus (evt2).\n",
      "Benchmarking expelliarmus (evt3).\n"
     ]
    }
   ],
   "source": [
    "# expelliarmus\n",
    "expelliarmus_times = []\n",
    "expelliarmus_sizes = []\n",
    "for encoding in raw_encodings:\n",
    "    print(f\"Benchmarking expelliarmus ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def expelliarmus_read() -> np.ndarray:\n",
    "        wizard = Wizard(encoding)\n",
    "        wizard.set_file(fpath)\n",
    "        return wizard.read(fpath)\n",
    "    assert hash(expelliarmus_read()) == reference_hash\n",
    "    expelliarmus_times.append(timeit.timeit(expelliarmus_read, number=REPEAT) / REPEAT)\n",
    "    expelliarmus_sizes.append(get_fsize_MiB(Path(fpath)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking HDF5 (hdf5).\n",
      "Benchmarking HDF5 (hdf5_lzf).\n",
      "Benchmarking HDF5 (hdf5_gzip).\n"
     ]
    }
   ],
   "source": [
    "# hdf5 variants\n",
    "hdf5_times = []\n",
    "hdf5_sizes = []\n",
    "for encoding in hdf5_encodings:\n",
    "    print(f\"Benchmarking HDF5 ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def hdf5_read() -> np.ndarray:\n",
    "        with h5py.File(fpath) as file:\n",
    "            return file[\"events\"][:] # type: ignore\n",
    "    assert hash(hdf5_read()) == reference_hash\n",
    "    hdf5_times.append(timeit.timeit(hdf5_read, number=REPEAT) / REPEAT)\n",
    "    hdf5_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Brotli (Q=1).\n",
      "Benchmarking Brotli (Q=6).\n",
      "Benchmarking Brotli (Q=11).\n"
     ]
    }
   ],
   "source": [
    "# brotli (UNDR)\n",
    "brotli_times = []\n",
    "brotli_sizes = []\n",
    "for quality in brotli_qualities:\n",
    "    print(f\"Benchmarking Brotli (Q={quality}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    def brotli_read() -> np.ndarray:\n",
    "        with open(fpath, \"rb\") as file:\n",
    "            return np.frombuffer(brotli.decompress(file.read()), dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "    assert hash(brotli_read()) == reference_hash\n",
    "    brotli_times.append(timeit.timeit(brotli_read, number=REPEAT) / REPEAT)\n",
    "    brotli_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking NumPy.\n"
     ]
    }
   ],
   "source": [
    "# numpy\n",
    "print(\"Benchmarking NumPy.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"numpy\")\n",
    "def numpy_read() -> np.ndarray:\n",
    "    return np.load(fpath)\n",
    "numpy_time = timeit.timeit(numpy_read, number=REPEAT) / REPEAT\n",
    "numpy_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking NumPy (UNDR).\n"
     ]
    }
   ],
   "source": [
    "# numpy (UNDR)\n",
    "print(\"Benchmarking NumPy (UNDR).\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "def undr_numpy_read() -> np.ndarray:\n",
    "    return np.fromfile(fpath, dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "assert hash(undr_numpy_read()) == reference_hash\n",
    "undr_numpy_time = timeit.timeit(undr_numpy_read, number=REPEAT) / REPEAT\n",
    "undr_numpy_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking AEDAT.\n"
     ]
    }
   ],
   "source": [
    "# aedat4\n",
    "print(\"Benchmarking AEDAT.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"aedat\")\n",
    "def aedat_read() -> np.ndarray:\n",
    "    decoder = aedat.Decoder(fpath) # type: ignore\n",
    "    return np.concatenate([packet[\"events\"] for packet in decoder if \"events\" in packet])\n",
    "assert hash(aedat_read(), normalize_time=True) == reference_hash\n",
    "aedat_time = timeit.timeit(aedat_read, number=REPEAT)/ REPEAT\n",
    "aedat_size = get_fsize_MiB(Path(fpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking loris.\n"
     ]
    }
   ],
   "source": [
    "# loris\n",
    "print(\"Benchmarking loris.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"loris\")\n",
    "def loris_read() -> np.ndarray:\n",
    "    return loris.read_file(fpath)[\"events\"] # type: ignore\n",
    "assert hash(loris_read()) == reference_hash\n",
    "loris_time = timeit.timeit(loris_read, number=REPEAT) / REPEAT\n",
    "loris_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking eventstream.\n"
     ]
    }
   ],
   "source": [
    "# eventstream\n",
    "print(\"Benchmarking eventstream.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "def eventstream_read() -> np.ndarray:\n",
    "    with event_stream.Decoder(fpath) as decoder:\n",
    "        return np.concatenate([packet for packet in decoder])\n",
    "assert hash(eventstream_read()) == reference_hash\n",
    "eventstream_time = timeit.timeit(eventstream_read, number=REPEAT) / REPEAT\n",
    "eventstream_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking aestream (dat).\n",
      "Benchmarking aestream (evt3).\n",
      "Decoded array does not correspond to original one for evt3!\n"
     ]
    }
   ],
   "source": [
    "# aestream\n",
    "aestream_times = list()\n",
    "aestream_sizes = list()\n",
    "for encoding in [\"dat\", \"evt3\"]:\n",
    "    print(f\"Benchmarking aestream ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def aestream_read() -> np.ndarray:\n",
    "        return aestream.FileInput(fpath, (640, 480)).load()\n",
    "    # Know bug: AEStream EVT3 decoding is wrong.\n",
    "    if hash(aestream_read()) != reference_hash:\n",
    "        print(f\"ERROR: Decoded array does not correspond to original one for {encoding}!\")\n",
    "    aestream_times.append(timeit.timeit(expelliarmus_read, number=REPEAT) / REPEAT)\n",
    "    aestream_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking DV.\n"
     ]
    }
   ],
   "source": [
    "import dv_processing as dv\n",
    "\n",
    "print(\"Benchmarking DV.\")\n",
    "gc.collect()\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "\n",
    "def dv_read() -> np.ndarray:\n",
    "    reader = dv.io.MonoCameraRecording(fpath)\n",
    "    event_slices = []\n",
    "    while reader.isRunning():\n",
    "        slice = reader.getNextEventBatch()\n",
    "        if slice is None:\n",
    "            break\n",
    "        event_slices.append(slice.numpy())\n",
    "    return np.concatenate(event_slices)\n",
    "\n",
    "dv_time = timeit.timeit(dv_read, number=REPEAT) / REPEAT\n",
    "dv_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "results = (\n",
    "    list(\n",
    "        zip(\n",
    "            raw_encodings,\n",
    "            [\"expelliarmus\"] * len(raw_encodings),\n",
    "            expelliarmus_times,\n",
    "            expelliarmus_sizes,\n",
    "        )\n",
    "    )\n",
    "    + list(\n",
    "        zip(\n",
    "            [\"dat\", \"evt3\"],\n",
    "            [\"AEStream\"] * 2,\n",
    "            aestream_times,\n",
    "            aestream_sizes,\n",
    "        )\n",
    "    )\n",
    "    + list(zip(hdf5_encodings, [\"h5py\"] * len(hdf5_encodings), hdf5_times, hdf5_sizes))\n",
    "    + list(\n",
    "        zip(\n",
    "            [f\"numpy/brotli (Q={quality})\" for quality in brotli_qualities],\n",
    "            [\"numpy/brotli\"] * len(brotli_qualities),\n",
    "            brotli_times,\n",
    "            brotli_sizes,\n",
    "        )\n",
    "    )\n",
    "    + [\n",
    "        (\"numpy (pickle)\", \"numpy\", numpy_time, numpy_size),\n",
    "        (\"numpy (UNDR)\", \"numpy\", undr_numpy_time, undr_numpy_size),\n",
    "        (\"aedat4\", \"aedat\", aedat_time, aedat_size),\n",
    "        (\"aedat4\", \"DV\", dv_time, dv_size),\n",
    "        (\"eventstream\", \"loris\", loris_time, loris_size),\n",
    "        (\"eventstream\", \"event_stream\", eventstream_time, eventstream_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open(\"results.json\", \"w\") as results_file:\n",
    "    json.dump(results, results_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot results\n",
    "\n",
    "import pandas\n",
    "import plotly.express\n",
    "\n",
    "with open(\"results.json\") as results_file:\n",
    "    results = json.load(results_file)\n",
    "dataframe = pandas.DataFrame(\n",
    "    {\n",
    "        \"Encoding\": [result[0] for result in results],\n",
    "        \"Framework\": [result[1] for result in results],\n",
    "        \"Read time [s]\": [result[2] for result in results],\n",
    "        \"File size [MiB]\": [result[3] for result in results],\n",
    "    }\n",
    ")\n",
    "\n",
    "title = f\"Reading the same {round(number_of_events / 1e6)} million events from different file formats.\"\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark.png\")\n",
    "\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=400, width=1000, margin=dict(l=10,r=10,b=10,t=10),)\n",
    "figure.write_image(\"file_read_benchmark_white.png\")\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark_log.png\", scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
