{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: expelliarmus in /Users/fab/miniconda3/lib/python3.11/site-packages (1.1.12)\n",
      "Requirement already satisfied: aedat in /Users/fab/miniconda3/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: event_stream in /Users/fab/miniconda3/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: h5py in /Users/fab/miniconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: aestream in /Users/fab/miniconda3/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: loris in /Users/fab/miniconda3/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: brotli in /Users/fab/miniconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: plotly in /Users/fab/miniconda3/lib/python3.11/site-packages (5.17.0)\n",
      "Requirement already satisfied: kaleido in /Users/fab/miniconda3/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: dv_processing in /Users/fab/miniconda3/lib/python3.11/site-packages (1.7.9)\n",
      "Requirement already satisfied: numpy in /Users/fab/miniconda3/lib/python3.11/site-packages (from expelliarmus) (1.25.2)\n",
      "Requirement already satisfied: pysdl2-dll in /Users/fab/miniconda3/lib/python3.11/site-packages (from aestream) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/fab/miniconda3/lib/python3.11/site-packages (from loris) (4.65.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/fab/miniconda3/lib/python3.11/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /Users/fab/miniconda3/lib/python3.11/site-packages (from plotly) (23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "NO SPECK module 'aestream.aestream_ext' has no attribute 'SpeckInput'\n"
     ]
    }
   ],
   "source": [
    "%pip install expelliarmus aedat event_stream h5py aestream loris brotli plotly kaleido dv_processing\n",
    "\n",
    "from expelliarmus import Wizard\n",
    "import aedat\n",
    "import event_stream\n",
    "import gc\n",
    "import hashlib\n",
    "import h5py\n",
    "import aestream\n",
    "import numpy as np\n",
    "import timeit\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import loris\n",
    "import brotli\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fname = \"driving_sample\"\n",
    "fname = \"construction\"  # use this one if you want to include aedat and eventstream benchmarks\n",
    "\n",
    "# where to download and generate all the benchmark data\n",
    "folder = Path(\"data/file-benchmark\")\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# key is the name of the encoding, value is the file name ending\n",
    "extension_map = {\n",
    "    \"aedat\": \".aedat4\",\n",
    "    \"dat\": \".dat\",\n",
    "    \"evt2\": \"_evt2.raw\",\n",
    "    \"evt3\": \"_evt3.raw\",\n",
    "    \"hdf5\": \".hdf5\",\n",
    "    \"hdf5_lzf\": \"_lzf.hdf5\",\n",
    "    \"hdf5_gzip\": \"_gzip.hdf5\",\n",
    "    \"numpy\": \".npy\",\n",
    "    \"loris\": \".es\",\n",
    "    \"eventstream\": \".es\",\n",
    "    \"brotli\": \".bin.br\",\n",
    "    \"undr_numpy\": \".dvs\",\n",
    "    \"undr_brotli_11\": \".11.dvs.br\",\n",
    "    \"undr_brotli_6\": \".6.dvs.br\",\n",
    "    \"undr_brotli_1\": \".1.dvs.br\",\n",
    "}\n",
    "get_fpath = lambda encoding: f\"{folder}/{fname}{extension_map[encoding]}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the 'base' files\n",
    "These are the files with the original data, which will be loaded and then converted to all other formats under test. Currently you can choose between events from a Prophesee raw evt3 or an aedat4 sample file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file_from_url(file_path, url):\n",
    "    print(f\"Downloading file to {file_path}... \")\n",
    "    r = requests.get(\n",
    "        url,\n",
    "        allow_redirects=True,\n",
    "    )\n",
    "    with open(f\"{file_path}.download\", \"wb\") as file:\n",
    "        file.write(r.content)\n",
    "    r.raise_for_status()\n",
    "    os.rename(f\"{file_path}.download\", file_path)\n",
    "    print(\"done!\")\n",
    "\n",
    "\n",
    "if fname == \"driving_sample\":\n",
    "    fpath = get_fpath(\"evt3\")\n",
    "    if not Path(fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            fpath, \"https://dataset.prophesee.ai/index.php/s/nVcLLdWAnNzrmII/download\"\n",
    "        )\n",
    "    wizard = Wizard(encoding=\"evt3\")\n",
    "    events_ti8_xi2_yi2_pu1 = wizard.read(fpath)\n",
    "\n",
    "\n",
    "if fname == \"construction\":\n",
    "    aedat_fpath = get_fpath(\"aedat\")\n",
    "    if not Path(aedat_fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            aedat_fpath,\n",
    "            \"https://cloudstor.aarnet.edu.au/plus/s/ORQ2oOz9NfwiHLZ/download?path=%2F&files=construction.aedat4\",\n",
    "        )\n",
    "    decoder = aedat.Decoder(aedat_fpath) # type: ignore\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for stream in decoder.id_to_stream().values():\n",
    "        if stream[\"type\"] == \"events\":\n",
    "            width = stream[\"width\"]\n",
    "            height = stream[\"height\"]\n",
    "            break\n",
    "    assert width != 0\n",
    "    assert height != 0\n",
    "    events_tu8_xu2_yu2_onb = np.concatenate(\n",
    "        [packet[\"events\"] for packet in decoder if \"events\" in packet]\n",
    "    )\n",
    "    assert np.count_nonzero(np.diff(events_tu8_xu2_yu2_onb[\"t\"].astype(\"<i8\")) < 0) == 0\n",
    "    events_tu8_xu2_yu2_onb[\"t\"] -= events_tu8_xu2_yu2_onb[\"t\"][0]\n",
    "    events_ti8_xi2_yi2_pu1 = events_tu8_xu2_yu2_onb.astype(\n",
    "        np.dtype([(\"t\", \"<i8\"), (\"x\", \"<i2\"), (\"y\", \"<i2\"), (\"p\", \"u1\")], align=True)\n",
    "    )\n",
    "    events_tu8_xu2_yu2_pu1 = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all comparison files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evt2 and dat\n",
    "raw_encodings = [\"dat\", \"evt2\", \"evt3\"]\n",
    "for encoding in raw_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        wizard = Wizard(encoding=encoding)\n",
    "        wizard.save(fpath=fpath, arr=events_ti8_xi2_yi2_pu1)\n",
    "\n",
    "# variants of hdf5\n",
    "hdf5_encodings = [\"hdf5\", \"hdf5_lzf\", \"hdf5_gzip\"]\n",
    "for encoding in hdf5_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        with h5py.File(fpath, \"w\") as fp:\n",
    "            print(f\"Generating {fpath}.\")\n",
    "            dataset_dict = dict(\n",
    "                name=\"events\",\n",
    "                shape=events_ti8_xi2_yi2_pu1.shape,\n",
    "                dtype=events_ti8_xi2_yi2_pu1.dtype,\n",
    "                data=events_ti8_xi2_yi2_pu1,\n",
    "            )\n",
    "            if encoding == \"hdf5\":\n",
    "                fp.create_dataset(**dataset_dict)\n",
    "            elif encoding == \"hdf5_lzf\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"lzf\")\n",
    "            elif encoding == \"hdf5_gzip\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"gzip\")\n",
    "\n",
    "# Event Stream\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    with event_stream.Encoder(fpath, \"dvs\", width, height) as encoder:\n",
    "        encoder.write(events_tu8_xu2_yu2_onb)\n",
    "\n",
    "# numpy (pickle)\n",
    "fpath = get_fpath(\"numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    np.save(fpath, events_ti8_xi2_yi2_pu1, allow_pickle=True)\n",
    "\n",
    "# numpy (UNDR)\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    events_tu8_xu2_yu2_pu1.tofile(fpath)\n",
    "\n",
    "# brotli (UNDR)\n",
    "brotli_qualities = [1, 6, 11]\n",
    "with open(get_fpath(\"undr_numpy\"), \"rb\") as uncompressed_file:\n",
    "    uncompressed_bytes = uncompressed_file.read()\n",
    "for quality in brotli_qualities:\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        compressed_bytes = brotli.compress(uncompressed_bytes, quality=quality)\n",
    "        with open(fpath, \"wb\") as compressed_file:\n",
    "            compressed_file.write(compressed_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aedat4\n",
    "# we do this in order to exclude IMU events that are part of the original file\n",
    "# see here for how to install dv https://dv-processing.inivation.com/rel_1.7/installation.html\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    import dv_processing as dv\n",
    "    from tqdm.notebook import tqdm\n",
    "    store = dv.EventStore()\n",
    "\n",
    "    aedat_compatible_events = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", int), (\"x\", int), (\"y\", int), (\"p\", bool)]))\n",
    "    for event in tqdm(aedat_compatible_events):\n",
    "        store.push_back(timestamp=event[\"t\"], x=event[\"x\"], y=event[\"y\"], polarity=event[\"p\"])\n",
    "\n",
    "    resolution = (aedat_compatible_events['x'].max()+1, aedat_compatible_events['y'].max()+1)\n",
    "    config = dv.io.MonoCameraWriter.EventOnlyConfig(\"DVXplorer_sample\", resolution)\n",
    "    writer = dv.io.MonoCameraWriter(fpath, config)\n",
    "    writer.writeEvents(store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPEAT = 20\n",
    "get_fsize_MiB = lambda fpath: round(fpath.stat().st_size / (1024 * 1024))\n",
    "\n",
    "def hash(events: np.ndarray, normalize_time: bool = False) -> str:\n",
    "    if normalize_time:\n",
    "        events[\"t\"] -= events[\"t\"][0]\n",
    "    result = hashlib.sha3_224()\n",
    "    result.update(events.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"on\", \"?\")])).tobytes())\n",
    "    return result.hexdigest()\n",
    "\n",
    "reference_hash = hash(events_tu8_xu2_yu2_onb)\n",
    "number_of_events = len(events_tu8_xu2_yu2_onb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete variables to minimize memory usage before the benchmarks\n",
    "\n",
    "del events_tu8_xu2_yu2_onb\n",
    "del events_ti8_xi2_yi2_pu1\n",
    "del events_tu8_xu2_yu2_pu1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking expelliarmus (dat).\n",
      "Benchmarking expelliarmus (evt2).\n",
      "Benchmarking expelliarmus (evt3).\n"
     ]
    }
   ],
   "source": [
    "# expelliarmus\n",
    "expelliarmus_times = []\n",
    "expelliarmus_sizes = []\n",
    "for encoding in raw_encodings:\n",
    "    print(f\"Benchmarking expelliarmus ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def expelliarmus_read() -> np.ndarray:\n",
    "        wizard = Wizard(encoding)\n",
    "        wizard.set_file(fpath)\n",
    "        return wizard.read(fpath)\n",
    "    assert hash(expelliarmus_read()) == reference_hash\n",
    "    expelliarmus_times.append(timeit.timeit(expelliarmus_read, number=REPEAT) / REPEAT)\n",
    "    expelliarmus_sizes.append(get_fsize_MiB(Path(fpath)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking HDF5 (hdf5).\n",
      "Benchmarking HDF5 (hdf5_lzf).\n",
      "Benchmarking HDF5 (hdf5_gzip).\n"
     ]
    }
   ],
   "source": [
    "# hdf5 variants\n",
    "hdf5_times = []\n",
    "hdf5_sizes = []\n",
    "for encoding in hdf5_encodings:\n",
    "    print(f\"Benchmarking HDF5 ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def hdf5_read() -> np.ndarray:\n",
    "        with h5py.File(fpath) as file:\n",
    "            return file[\"events\"][:] # type: ignore\n",
    "    assert hash(hdf5_read()) == reference_hash\n",
    "    hdf5_times.append(timeit.timeit(hdf5_read, number=REPEAT) / REPEAT)\n",
    "    hdf5_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Brotli (Q=1).\n",
      "Benchmarking Brotli (Q=6).\n",
      "Benchmarking Brotli (Q=11).\n"
     ]
    }
   ],
   "source": [
    "# brotli (UNDR)\n",
    "brotli_times = []\n",
    "brotli_sizes = []\n",
    "for quality in brotli_qualities:\n",
    "    print(f\"Benchmarking Brotli (Q={quality}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    def brotli_read() -> np.ndarray:\n",
    "        with open(fpath, \"rb\") as file:\n",
    "            return np.frombuffer(brotli.decompress(file.read()), dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "    assert hash(brotli_read()) == reference_hash\n",
    "    brotli_times.append(timeit.timeit(brotli_read, number=REPEAT) / REPEAT)\n",
    "    brotli_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking NumPy.\n"
     ]
    }
   ],
   "source": [
    "# numpy\n",
    "print(\"Benchmarking NumPy.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"numpy\")\n",
    "def numpy_read() -> np.ndarray:\n",
    "    return np.load(fpath)\n",
    "numpy_time = timeit.timeit(numpy_read, number=REPEAT) / REPEAT\n",
    "numpy_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking NumPy (UNDR).\n"
     ]
    }
   ],
   "source": [
    "# numpy (UNDR)\n",
    "print(\"Benchmarking NumPy (UNDR).\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "def undr_numpy_read() -> np.ndarray:\n",
    "    return np.fromfile(fpath, dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "assert hash(undr_numpy_read()) == reference_hash\n",
    "undr_numpy_time = timeit.timeit(undr_numpy_read, number=REPEAT) / REPEAT\n",
    "undr_numpy_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking AEDAT.\n"
     ]
    }
   ],
   "source": [
    "# aedat4\n",
    "print(\"Benchmarking AEDAT.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"aedat\")\n",
    "def aedat_read() -> np.ndarray:\n",
    "    decoder = aedat.Decoder(fpath) # type: ignore\n",
    "    return np.concatenate([packet[\"events\"] for packet in decoder if \"events\" in packet])\n",
    "assert hash(aedat_read(), normalize_time=True) == reference_hash\n",
    "aedat_time = timeit.timeit(aedat_read, number=REPEAT)/ REPEAT\n",
    "aedat_size = get_fsize_MiB(Path(fpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking loris.\n"
     ]
    }
   ],
   "source": [
    "# loris\n",
    "print(\"Benchmarking loris.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"loris\")\n",
    "def loris_read() -> np.ndarray:\n",
    "    return loris.read_file(fpath)[\"events\"] # type: ignore\n",
    "assert hash(loris_read()) == reference_hash\n",
    "loris_time = timeit.timeit(loris_read, number=REPEAT) / REPEAT\n",
    "loris_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking eventstream.\n"
     ]
    }
   ],
   "source": [
    "# eventstream\n",
    "print(\"Benchmarking eventstream.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "def eventstream_read() -> np.ndarray:\n",
    "    with event_stream.Decoder(fpath) as decoder:\n",
    "        return np.concatenate([packet for packet in decoder])\n",
    "assert hash(eventstream_read()) == reference_hash\n",
    "eventstream_time = timeit.timeit(eventstream_read, number=REPEAT) / REPEAT\n",
    "eventstream_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking aestream (dat).\n",
      "Benchmarking aestream (evt3).\n",
      "ERROR: Decoded array does not correspond to the original one for evt3!\n"
     ]
    }
   ],
   "source": [
    "# aestream\n",
    "aestream_times = list()\n",
    "aestream_sizes = list()\n",
    "\n",
    "EVT3_AEStream_works = True\n",
    "for encoding in [\"dat\", \"evt3\"]:\n",
    "    print(f\"Benchmarking aestream ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def aestream_read() -> np.ndarray:\n",
    "        return aestream.FileInput(fpath, (640, 480)).load()\n",
    "    # Know bug: AEStream EVT3 decoding is wrong.\n",
    "    if hash(aestream_read()) != reference_hash:\n",
    "        print(f\"ERROR: Decoded array does not correspond to the original one for {encoding}!\")\n",
    "        EVT3_AEStream_works = True\n",
    "        continue\n",
    "    aestream_times.append(timeit.timeit(aestream_read, number=REPEAT) / REPEAT)\n",
    "    aestream_sizes.append(get_fsize_MiB(Path(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking DV.\n"
     ]
    }
   ],
   "source": [
    "import dv_processing as dv\n",
    "\n",
    "print(\"Benchmarking DV.\")\n",
    "gc.collect()\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "\n",
    "def dv_read() -> np.ndarray:\n",
    "    reader = dv.io.MonoCameraRecording(fpath)\n",
    "    event_slices = []\n",
    "    while reader.isRunning():\n",
    "        slice = reader.getNextEventBatch()\n",
    "        if slice is None:\n",
    "            break\n",
    "        event_slices.append(slice.numpy())\n",
    "    return np.concatenate(event_slices)\n",
    "\n",
    "dv_time = timeit.timeit(dv_read, number=REPEAT) / REPEAT\n",
    "dv_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "results = (\n",
    "    list(\n",
    "        zip(\n",
    "            raw_encodings,\n",
    "            [\"expelliarmus\"] * len(raw_encodings),\n",
    "            expelliarmus_times,\n",
    "            expelliarmus_sizes,\n",
    "        )\n",
    "    )\n",
    "    + (\n",
    "        list(zip([\"dat\", \"evt3\"], [\"AEStream\"] * 2, aestream_times, aestream_sizes)) \n",
    "        if EVT3_AEStream_works else \n",
    "        list(\"dat\", \"AEStream\", aestream_times[0], aestream_sizes[0])\n",
    "    )\n",
    "    + list(zip(hdf5_encodings, [\"h5py\"] * len(hdf5_encodings), hdf5_times, hdf5_sizes))\n",
    "    + list(\n",
    "        zip(\n",
    "            [f\"numpy/brotli (Q={quality})\" for quality in brotli_qualities],\n",
    "            [\"numpy/brotli\"] * len(brotli_qualities),\n",
    "            brotli_times,\n",
    "            brotli_sizes,\n",
    "        )\n",
    "    )\n",
    "    + [\n",
    "        (\"numpy (pickle)\", \"numpy\", numpy_time, numpy_size),\n",
    "        (\"numpy (UNDR)\", \"numpy\", undr_numpy_time, undr_numpy_size),\n",
    "        (\"aedat4\", \"aedat\", aedat_time, aedat_size),\n",
    "        (\"aedat4\", \"DV\", dv_time, dv_size),\n",
    "        (\"eventstream\", \"loris\", loris_time, loris_size),\n",
    "        (\"eventstream\", \"event_stream\", eventstream_time, eventstream_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open(\"results.json\", \"w\") as results_file:\n",
    "    json.dump(results, results_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m results_file:\n\u001b[1;32m      7\u001b[0m     results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(results_file)\n\u001b[0;32m----> 8\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEncoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFramework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRead time [s]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFile size [MiB]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading the same \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(number_of_events\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m million events from different file formats.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m figure \u001b[38;5;241m=\u001b[39m plotly\u001b[38;5;241m.\u001b[39mexpress\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m     20\u001b[0m     dataframe,\n\u001b[1;32m     21\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead time [s]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     title\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "## Plot results\n",
    "\n",
    "import pandas\n",
    "import plotly.express\n",
    "\n",
    "with open(\"results.json\") as results_file:\n",
    "    results = json.load(results_file)\n",
    "dataframe = pandas.DataFrame(\n",
    "    {\n",
    "        \"Encoding\": [result[0] for result in results],\n",
    "        \"Framework\": [result[1] for result in results],\n",
    "        \"Read time [s]\": [result[2] for result in results],\n",
    "        \"File size [MiB]\": [result[3] for result in results],\n",
    "    }\n",
    ")\n",
    "\n",
    "title = f\"Reading the same {round(number_of_events / 1e6)} million events from different file formats.\"\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark.png\")\n",
    "\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=400, width=1000, margin=dict(l=10,r=10,b=10,t=10),)\n",
    "figure.write_image(\"file_read_benchmark_white.png\")\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark_log.png\", scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
